# Import necessary libraries
import asyncio  # For handling asynchronous operations
import os       # For environment variable access
import sys      # For system-specific parameters and functions
import json     # For handling JSON data (used for tool arguments and tool outputs)

# Import MCP client components
from typing import Optional  # For type hinting optional values
from contextlib import AsyncExitStack  # For managing multiple async tasks
from mcp import ClientSession, StdioServerParameters  # MCP session management
from mcp.client.stdio import stdio_client  # MCP client for standard I/O communication

# Import OpenAI SDK
from openai import OpenAI

from dotenv import load_dotenv  # For loading API keys from a .env file

# Load environment variables from .env file
load_dotenv()


class MCPClient:
    def __init__(self):
        """Initialize the MCP client and configure the OpenAI API."""
        self.session: Optional[ClientSession] = None  # MCP session for communication
        self.exit_stack = AsyncExitStack()  # Manages async resource cleanup

        # Retrieve the OpenAI API key from environment variables
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY not found. Please add it to your .env file.")

        # Configure the OpenAI client
        self.openai_client = OpenAI(api_key=openai_api_key)

        # Use a cheaper OpenAI model
        self.model_name = "gpt-4o-mini"

        # Will be filled after connecting to MCP server
        self.tools = []

    async def connect_to_server(self, server_script_path: str):
        """Connect to the MCP server and list available tools."""

        # Determine whether the server script is written in Python or JavaScript
        command = sys.executable if server_script_path.endswith(".py") else "node"

        # Define the parameters for connecting to the MCP server
        server_params = StdioServerParameters(command=command, args=[server_script_path])

        # Establish communication with the MCP server using standard input/output (stdio)
        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))

        # Extract the read/write streams from the transport object
        self.stdio, self.write = stdio_transport

        # Initialize the MCP client session, which allows interaction with the server
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))

        # Send an initialization request to the MCP server
        await self.session.initialize()

        # Request the list of available tools from the MCP server
        response = await self.session.list_tools()
        mcp_tools = response.tools  # Extract the tool list from the response

        # Print a message showing the names of the tools available on the server
        print("\nConnected to server with tools:", [tool.name for tool in mcp_tools])

        # Convert MCP tools to OpenAI "tools" format
        self.tools = convert_mcp_tools_to_openai(mcp_tools)

    async def process_query(self, query: str) -> str:
        """
        Process a user query using the OpenAI API and execute tool calls if needed.

        Args:
            query (str): The user's input query.

        Returns:
            str: The response generated by the OpenAI model.
        """

        # Create a running input list we will add to over time
        input_list = [{"role": "user", "content": query}]

        while True:
            # Send user input to OpenAI and include available tools for function calling
            response = self.openai_client.responses.create(
                model=self.model_name,
                tools=self.tools,
                input=input_list,
            )

            # If the model returned tool calls, execute them and feed outputs back
            tool_calls_found = False

            for item in response.output:
                # Some models may include non-text items; keep them if needed
                if getattr(item, "type", None) == "reasoning":
                    # If present, pass reasoning items back when continuing a tool-call chain
                    input_list.append(item.model_dump())
                    continue

                if getattr(item, "type", None) == "function_call":
                    tool_calls_found = True
                    tool_name = item.name
                    tool_args_raw = item.arguments or "{}"

                    try:
                        tool_args = json.loads(tool_args_raw)
                    except Exception:
                        tool_args = {}

                    print(f"\n[OpenAI requested tool call: {tool_name} args={tool_args}]")

                    input_list.append(item.model_dump())

                    # Execute the tool using the MCP server
                    try:
                        result = await self.session.call_tool(tool_name, tool_args)

                        # Normalize MCP result into plain text
                        tool_text = ""
                        if hasattr(result, "content"):
                            if isinstance(result.content, list):
                                tool_text = "\n".join(
                                    getattr(part, "text", str(part)) for part in result.content
                                )
                            else:
                                tool_text = str(result.content)
                        else:
                            tool_text = str(result)

                        output_payload = json.dumps({"result": tool_text})
                    except Exception as e:
                        output_payload = json.dumps({"error": str(e)})

                    # Provide function call results back to the model
                    input_list.append(
                        {
                            "type": "function_call_output",
                            "call_id": item.call_id,
                            "output": output_payload,
                        }
                    )

            # If no tool calls were requested, return the model's text output
            if not tool_calls_found:
                return response.output_text or ""

            # Otherwise, loop again: model sees tool outputs and may respond or request more tools

    async def chat_loop(self):
        """Run an interactive chat session with the user."""
        print("\nMCP Client Started! Type 'quit' to exit.")

        while True:
            query = input("\nQuery: ").strip()
            if query.lower() == "quit":
                break

            # Process the user's query and display the response
            response = await self.process_query(query)
            print("\n" + response)

    async def cleanup(self):
        """Clean up resources before exiting."""
        await self.exit_stack.aclose()


def clean_schema(schema):
    """
    Recursively removes 'title' fields from the JSON schema.

    Args:
        schema (dict): The schema dictionary.

    Returns:
        dict: Cleaned schema without 'title' fields.
    """
    if isinstance(schema, dict):
        schema.pop("title", None)

        # Recursively clean nested structures
        for k, v in list(schema.items()):
            schema[k] = clean_schema(v)

        return schema

    if isinstance(schema, list):
        return [clean_schema(x) for x in schema]

    return schema


def convert_mcp_tools_to_openai(mcp_tools):
    """
    Converts MCP tool definitions to the correct format for OpenAI function calling.

    Args:
        mcp_tools (list): List of MCP tool objects with 'name', 'description', and 'inputSchema'.

    Returns:
        list: List of OpenAI tools in the Responses API format.
    """
    openai_tools = []

    for tool in mcp_tools:
        parameters = clean_schema(tool.inputSchema)

        openai_tools.append(
            {
                "type": "function",
                "name": tool.name,
                "description": tool.description,
                "parameters": parameters,
            }
        )

    return openai_tools


async def main():
    """Main function to start the MCP client."""
    if len(sys.argv) < 2:
        print("Usage: python gemini_client.py <path_to_server_script>")
        sys.exit(1)

    client = MCPClient()
    try:
        # Connect to the MCP server and start the chat loop
        await client.connect_to_server(sys.argv[1])
        await client.chat_loop()
    finally:
        # Ensure resources are cleaned up
        await client.cleanup()


if __name__ == "__main__":
    # Run the main function within the asyncio event loop
    asyncio.run(main())
